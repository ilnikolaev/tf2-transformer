{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21786fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c6147",
   "metadata": {},
   "source": [
    "Simple linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07553c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.Module):\n",
    "    def __init__(self, input_dim, output_dim, name=\"Linear\"):\n",
    "        super().__init__(name=name)\n",
    "        self.initializer = tf.initializers.GlorotUniform()\n",
    "        self.w = tf.Variable(self.initializer(shape=[input_dim, output_dim]), name=name + \"_w\")\n",
    "        self.b = tf.Variable(tf.zeros([output_dim]), name=name+\"_b\")\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x):\n",
    "        return tf.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25656011",
   "metadata": {},
   "source": [
    "Gaussan Error Linear Unit activation for FFN.\n",
    "Same as tf.keras.activations.Gelu(x, approximate=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gelu_new(x):\n",
    "    return 0.5*x*(1+tf.math.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.math.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba171dcb",
   "metadata": {},
   "source": [
    "Create triangular mask for decoder layer, to make sure its attention uses only previous tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebf2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def CasualMask(size): \n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879979a",
   "metadata": {},
   "source": [
    "FeedForwardNetwork. Make transformer block attention outputs fit to the input of the next transformer block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(tf.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim, name=\"FeedForward\"):\n",
    "        super().__init__(name=name)\n",
    "        self.dense0 = Linear(embed_dim, ffn_dim, name=name)\n",
    "        self.dense1 = Linear(ffn_dim, embed_dim, name=name)\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x):\n",
    "        out1 = self.dense0(x)\n",
    "        out2 = gelu_new(out1)\n",
    "        return self.dense1(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ccae83",
   "metadata": {},
   "source": [
    "Calculate attention outputs and weights. The mask is multiplied by -1e9 before softmax to assign zero weights for useless tokens that are marked with 1. Setting weights after softmax breaks the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    attention_logits = qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)\n",
    "        \n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab694a4",
   "metadata": {},
   "source": [
    "Multi Head Attention layer. This layer splits the Q, K, V into separate heads. Each value has a dimension divided by the number of heads after splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(tf.Module):\n",
    "    def __init__(self, num_heads, key_dim, name=\"MHA\"):\n",
    "        super().__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.depth = key_dim // num_heads\n",
    "        \n",
    "        self.wq = Linear(key_dim, key_dim, name=\"MHA_query\")\n",
    "        self.wk = Linear(key_dim, key_dim, name=\"MHA_key\")\n",
    "        self.wv = Linear(key_dim, key_dim, name=\"MHA_value\")\n",
    "        \n",
    "        self.dense = Linear(key_dim, key_dim, name=\"MHA_dense\")\n",
    "        \n",
    "    @tf.function\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, - 1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0,2,1,3])\n",
    "    \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.key_dim))\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216a09e",
   "metadata": {},
   "source": [
    "Token and postitional embedding. If it is needed, this layer also returns the padding mask used in the scaled dot product attention function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, maxlen, name=\"Embeddings\"):\n",
    "        super().__init__(name=name)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.initializer = tf.initializers.GlorotUniform()\n",
    "        \n",
    "        self.w0 = tf.Variable(self.initializer([vocab_size, embed_dim]),name=\"token_embedding\")\n",
    "        self.w1 = tf.Variable(self.initializer([maxlen, embed_dim]),name=\"pos_embedding\")\n",
    "\n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, masking=False):\n",
    "        x = tf.cast(x, tf.int32)\n",
    "        seq_len = tf.shape(x)[-1]\n",
    "        \n",
    "        if isinstance(x, tf.sparse.SparseTensor):\n",
    "            sparse_inputs_expanded = tf.sparse.expand_dims(x, axis=-1)\n",
    "            out = tf.nn.safe_embedding_lookup_sparse(embedding_weights=self.w0,sparse_ids=sparse_inputs_expanded, default_id=0)\n",
    "            \n",
    "        else:\n",
    "            out = tf.nn.embedding_lookup(self.w0, x)\n",
    "\n",
    "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        pos_out = tf.nn.embedding_lookup(self.w1, positions)\n",
    "        \n",
    "        if masking:\n",
    "            #Create mask to prevent encoder paying attention to PAD tokens\n",
    "            mask = tf.cast(tf.math.equal(x, 0)[:, tf.newaxis, tf.newaxis, :], tf.float32)\n",
    "            return out + pos_out, mask\n",
    "        \n",
    "        return out + pos_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a17daa",
   "metadata": {},
   "source": [
    "Layer normalization. Normalize to mean = 0, std = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(tf.Module):\n",
    "    def __init__(self, epsilon=1e-5, axis=-1, name=\"LayerNorm\"):\n",
    "        super().__init__(name=name)\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.epsilon = epsilon\n",
    "        self.axis = axis\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x):\n",
    "        n_state = x.shape[-1]\n",
    "        if self.gamma is None and self.beta is None:\n",
    "            self.gamma = tf.Variable(tf.ones(n_state), name=\"layernorm_gamma\")\n",
    "            self.beta = tf.Variable(tf.zeros(n_state), name=\"layernorm_beta\")\n",
    "        \n",
    "        u = tf.reduce_mean(x, axis=self.axis, keepdims=True)\n",
    "        s = tf.reduce_mean(tf.square(x-u), axis=self.axis, keepdims=True)\n",
    "        x = (x - u) * tf.math.rsqrt(s + self.epsilon)\n",
    "        return x * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237de794",
   "metadata": {},
   "source": [
    "Create encoder block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.Module):\n",
    "    def __init__(self, num_heads, embed_dim, ffn_dim, name=\"encoder\"):\n",
    "        super().__init__(name=name)\n",
    "        self.att = MHA(num_heads, embed_dim)\n",
    "        self.ffn = FFN(embed_dim, ffn_dim)\n",
    "        self.norm1 = LayerNorm(epsilon=1e-5)\n",
    "        self.norm2 = LayerNorm(epsilon=1e-5)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, mask=None):\n",
    "        att_output, att_weights = self.att(x,x,x, mask)\n",
    "        att_output = tf.nn.dropout(att_output, rate=0.1)\n",
    "        norm_output = self.norm1(x + att_output)\n",
    "        ffn_out = self.ffn(norm_output)\n",
    "        ffn_out = tf.nn.dropout(ffn_out, rate=0.1)\n",
    "        \n",
    "        return self.norm2(norm_output + ffn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e2d22",
   "metadata": {},
   "source": [
    "Create decoder block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc50c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.Module):\n",
    "    def __init__(self, num_heads, embed_dim, ffn_dim, name=\"decoder\"):\n",
    "        super().__init__(name=name)\n",
    "        self.att1 = MHA(num_heads, embed_dim)\n",
    "        self.att2 = MHA(num_heads, embed_dim)\n",
    "        self.ffn = FFN(embed_dim, ffn_dim)\n",
    "        \n",
    "        self.layernorm1 = LayerNorm(epsilon=1e-5)\n",
    "        self.layernorm2 = LayerNorm(epsilon=1e-5)\n",
    "        self.layernorm3 = LayerNorm(epsilon=1e-5)\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, encoder_seq, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        casual_mask = CasualMask(seq_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = tf.maximum(tf.cast(mask, tf.float32), casual_mask)\n",
    "            \n",
    "        att1_output, att1_weights = self.att1(x, x, x, casual_mask)\n",
    "        att1_dropout = tf.nn.dropout(att1_output, rate=0.1)\n",
    "        out1 = self.layernorm1(x+att1_dropout)\n",
    "        \n",
    "        att2_output, att2_weights = self.att2(out1, encoder_seq, encoder_seq, mask)\n",
    "        att2_dropout = tf.nn.dropout(att2_output, rate=0.1)\n",
    "        out2 = self.layernorm2(out1 + att2_dropout)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_dropout = tf.nn.dropout(ffn_output, rate=0.1)\n",
    "        return self.layernorm3 (out2+ffn_dropout), att1_weights, att2_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cf855",
   "metadata": {},
   "source": [
    "These layers makes it possible to use multiple encoder and decoder layers by iterating over the specified number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_layer(tf.Module):\n",
    "    def __init__(self, num_layers, num_heads, embed_dim, ffn_dim, name=\"encoder_layer\"):\n",
    "        super().__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [Encoder(num_heads, embed_dim, ffn_dim, name=\"encoder\"+str(_)) #Every block needs a name\n",
    "                    for _ in range(num_layers)]                                          #Or it will not be recognized\n",
    "                                                                                         #After loading model from \n",
    "                                                                                         #tf.saved_model\n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, mask=None):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47728a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_layer(tf.Module):\n",
    "    def __init__(self, num_layers, num_heads, embed_dim, ffn_dim, name=\"decoder_layer\"):\n",
    "        super().__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [Decoder(num_heads, embed_dim, ffn_dim, name=\"decoder\"+str(_))\n",
    "                          for _ in range(num_layers)]\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, encoder_seq, mask=None):\n",
    "        attention_weights = {}\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, encoder_seq, mask)\n",
    "            \n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "            \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63206c57",
   "metadata": {},
   "source": [
    "Define arguments for building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0899578",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "num_heads = 8\n",
    "ffn_dim = 3072\n",
    "embed_dim = 768\n",
    "vocab_size = 31000\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7927c46",
   "metadata": {},
   "source": [
    "Build the transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdfddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.Module):\n",
    "    def __init__(self, num_layers, num_heads, ffn_dim, embed_dim, vocab_size, maxlen, name=\"transformer\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dense = Linear(embed_dim, vocab_size, name=\"model_output\")\n",
    "        \n",
    "        self.embedding_encoder = Embedding(vocab_size, embed_dim, maxlen)\n",
    "        self.embedding_decoder = Embedding(vocab_size, embed_dim, maxlen)\n",
    "        \n",
    "        self.encoder = Encoder_layer(num_layers, num_heads, embed_dim, ffn_dim)\n",
    "        self.decoder = Decoder_layer(num_layers, num_heads, embed_dim, ffn_dim)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, maxlen], dtype=tf.int32), tf.TensorSpec(shape=[None,maxlen], dtype=tf.int32)])\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, inp, targ):\n",
    "        encoder_emb_output, encoder_pad_mask = self.embedding_encoder(inp, masking=True)\n",
    "        encoder_output = self.encoder(encoder_emb_output, mask=encoder_pad_mask)\n",
    "        \n",
    "        decoder_emb_output = self.embedding_decoder(targ)\n",
    "        decoder_output, att_weights = self.decoder(decoder_emb_output, encoder_output, mask=encoder_pad_mask)\n",
    "\n",
    "        return self.dense(decoder_output), att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca3107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, num_heads, ffn_dim, embed_dim, vocab_size, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29961dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(transformer, \"transformer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
