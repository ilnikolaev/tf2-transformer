{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21786fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c6147",
   "metadata": {},
   "source": [
    "Simple linear layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07553c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.Module):\n",
    "    def __init__(self, input_dim, output_dim, name=\"Linear\"):\n",
    "        super().__init__(name=name)\n",
    "        self.initializer = tf.initializers.GlorotUniform()\n",
    "        self.w = tf.Variable(self.initializer(shape=[input_dim, output_dim]), name=name + \"_w\")\n",
    "        self.b = tf.Variable(tf.zeros([output_dim]), name=name+\"_b\")\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x):\n",
    "        return tf.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25656011",
   "metadata": {},
   "source": [
    "Gaussan Error Linear Unit activation for FFN.\n",
    "Same as tf.keras.activations.Gelu(x, approximate=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gelu_new(x):\n",
    "    return 0.5*x*(1+tf.math.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.math.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba171dcb",
   "metadata": {},
   "source": [
    "Create triangular mask for decoder layer, to make sure its attention uses only previous tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebf2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def CasualMask(size): \n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879979a",
   "metadata": {},
   "source": [
    "FeedForwardNetwork. Make transformer block attention outputs fit to the input of the next transformer block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(tf.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim, name=\"FeedForward\"):\n",
    "        super().__init__(name=name)\n",
    "        self.dense0 = Linear(embed_dim, ffn_dim, name=name)\n",
    "        self.dense1 = Linear(ffn_dim, embed_dim, name=name)\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x):\n",
    "        out1 = self.dense0(x)\n",
    "        out2 = gelu_new(out1)\n",
    "        return self.dense1(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ccae83",
   "metadata": {},
   "source": [
    "Calculate attention outputs and weights. The mask is multiplied by -1e9 before softmax to assign zero weights for useless tokens that are marked with 1. Setting weights after softmax breaks the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    attention_logits = qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        attention_logits += (mask * -1e9)\n",
    "        \n",
    "    attention_weights = tf.nn.softmax(attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab694a4",
   "metadata": {},
   "source": [
    "Multi Head Attention layer. This layer splits the Q, K, V into separate heads. Each value has a dimension divided by the number of heads after splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(tf.Module):\n",
    "    def __init__(self, num_heads, key_dim, name=\"MHA\"):\n",
    "        super().__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.depth = key_dim // num_heads\n",
    "        \n",
    "        self.wq = Linear(key_dim, key_dim, name=\"MHA_query\")\n",
    "        self.wk = Linear(key_dim, key_dim, name=\"MHA_key\")\n",
    "        self.wv = Linear(key_dim, key_dim, name=\"MHA_value\")\n",
    "        \n",
    "        self.dense = Linear(key_dim, key_dim, name=\"MHA_dense\")\n",
    "        \n",
    "    @tf.function\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, - 1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0,2,1,3])\n",
    "    \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.key_dim))\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216a09e",
   "metadata": {},
   "source": [
    "Token and postitional embedding. If it is needed, this layer also returns the padding mask used in the scaled dot product attention function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, maxlen, name=\"Embeddings\"):\n",
    "        super().__init__(name=name)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.initializer = tf.initializers.GlorotUniform()\n",
    "        \n",
    "        self.w0 = tf.Variable(self.initializer([vocab_size, embed_dim]),name=\"token_embedding\")\n",
    "        self.pos_encoding = positional_encoding(maxlen,self.embed_dim)\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, masking=False):\n",
    "        x = tf.cast(x, tf.int32)\n",
    "        seq_len = tf.shape(x)[-1]\n",
    "        \n",
    "        if isinstance(x, tf.sparse.SparseTensor):\n",
    "            sparse_inputs_expanded = tf.sparse.expand_dims(x, axis=-1)\n",
    "            out = tf.nn.safe_embedding_lookup_sparse(embedding_weights=self.w0,sparse_ids=sparse_inputs_expanded, default_id=0)\n",
    "            \n",
    "        else:\n",
    "            out = tf.nn.embedding_lookup(self.w0, x)\n",
    "\n",
    "        out *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        out += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        if masking:\n",
    "            #Create mask to prevent encoder paying attention to PAD tokens\n",
    "            mask = tf.cast(tf.math.equal(x, 0)[:, tf.newaxis, tf.newaxis, :], tf.float32)\n",
    "            return out, mask\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a17daa",
   "metadata": {},
   "source": [
    "Layer normalization. Normalize to mean = 0, std = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(tf.Module):\n",
    "    def __init__(self, shape, epsilon=1e-5, axis=-1, name=\"LayerNorm\"):\n",
    "        super().__init__(name=name)\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.epsilon = epsilon\n",
    "        self.axis = axis\n",
    "        self.gamma = tf.Variable(tf.ones(shape), name=\"layernorm_gamma\")\n",
    "        self.beta = tf.Variable(tf.zeros(shape), name=\"layernorm_beta\")\n",
    "    \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x):\n",
    "        u = tf.reduce_mean(x, axis=self.axis, keepdims=True)\n",
    "        s = tf.reduce_mean(tf.square(x-u), axis=self.axis, keepdims=True)\n",
    "        x = (x - u) * tf.math.rsqrt(s + self.epsilon)\n",
    "        return x * self.gamma + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237de794",
   "metadata": {},
   "source": [
    "Create encoder block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.Module):\n",
    "    def __init__(self, num_heads, embed_dim, ffn_dim, name=\"encoder\"):\n",
    "        super().__init__(name=name)\n",
    "        self.att = MHA(num_heads, embed_dim)\n",
    "        self.ffn = FFN(embed_dim, ffn_dim)\n",
    "        self.norm1 = LayerNorm(embed_dim)\n",
    "        self.norm2 = LayerNorm(embed_dim)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, mask=None):\n",
    "        att_output, att_weights = self.att(x,x,x, mask)\n",
    "        att_output = tf.nn.dropout(att_output, rate=0.1)\n",
    "        norm_output = self.norm1(x + att_output)\n",
    "        ffn_out = self.ffn(norm_output)\n",
    "        ffn_out = tf.nn.dropout(ffn_out, rate=0.1)\n",
    "        \n",
    "        return self.norm2(norm_output + ffn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e2d22",
   "metadata": {},
   "source": [
    "Create decoder block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc50c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.Module):\n",
    "    def __init__(self, num_heads, embed_dim, ffn_dim, name=\"decoder\"):\n",
    "        super().__init__(name=name)\n",
    "        self.att1 = MHA(num_heads, embed_dim)\n",
    "        self.att2 = MHA(num_heads, embed_dim)\n",
    "        self.ffn = FFN(embed_dim, ffn_dim)\n",
    "        \n",
    "        self.layernorm1 = LayerNorm(embed_dim)\n",
    "        self.layernorm2 = LayerNorm(embed_dim)\n",
    "        self.layernorm3 = LayerNorm(embed_dim)\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, encoder_seq, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        casual_mask = CasualMask(seq_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = tf.maximum(tf.cast(mask, tf.float32), casual_mask)\n",
    "            \n",
    "        att1_output, att1_weights = self.att1(x, x, x, casual_mask)\n",
    "        att1_dropout = tf.nn.dropout(att1_output, rate=0.1)\n",
    "        out1 = self.layernorm1(x+att1_dropout)\n",
    "        \n",
    "        att2_output, att2_weights = self.att2(out1, encoder_seq, encoder_seq, mask)\n",
    "        att2_dropout = tf.nn.dropout(att2_output, rate=0.1)\n",
    "        out2 = self.layernorm2(out1 + att2_dropout)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_dropout = tf.nn.dropout(ffn_output, rate=0.1)\n",
    "        return self.layernorm3 (out2+ffn_dropout), att1_weights, att2_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cf855",
   "metadata": {},
   "source": [
    "These layers makes it possible to use multiple encoder and decoder layers by iterating over the specified number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_layer(tf.Module):\n",
    "    def __init__(self, num_layers, num_heads, embed_dim, ffn_dim, name=\"encoder_layer\"):\n",
    "        super().__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [Encoder(num_heads, embed_dim, ffn_dim, name=\"encoder\"+str(_)) #Every block needs a name\n",
    "                    for _ in range(num_layers)]                                          #Or it will not be recognized\n",
    "                                                                                         #After loading model from \n",
    "                                                                                         #tf.saved_model\n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, mask=None):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47728a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_layer(tf.Module):\n",
    "    def __init__(self, num_layers, num_heads, embed_dim, ffn_dim, name=\"decoder_layer\"):\n",
    "        super().__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [Decoder(num_heads, embed_dim, ffn_dim, name=\"decoder\"+str(_))\n",
    "                          for _ in range(num_layers)]\n",
    "        \n",
    "    @tf.function\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, x, encoder_seq, mask=None):\n",
    "        attention_weights = {}\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, encoder_seq, mask)\n",
    "            \n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "            \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63206c57",
   "metadata": {},
   "source": [
    "Define hyperparameters for building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0899578",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "num_heads = 8\n",
    "ffn_dim = 2048\n",
    "embed_dim = 512\n",
    "vocab_size = 31000\n",
    "maxlen = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7927c46",
   "metadata": {},
   "source": [
    "Build the transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdfddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.Module):\n",
    "    def __init__(self, num_layers, num_heads, ffn_dim, embed_dim, vocab_size, maxlen, name=\"transformer\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dense = Linear(embed_dim, vocab_size, name=\"model_output\")\n",
    "        \n",
    "        self.embedding_encoder = Embedding(vocab_size, embed_dim, maxlen)\n",
    "        self.embedding_decoder = Embedding(vocab_size, embed_dim, maxlen)\n",
    "        \n",
    "        self.encoder = Encoder_layer(num_layers, num_heads, embed_dim, ffn_dim)\n",
    "        self.decoder = Decoder_layer(num_layers, num_heads, embed_dim, ffn_dim)\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, maxlen], dtype=tf.int32), tf.TensorSpec(shape=[None,maxlen], dtype=tf.int32)])\n",
    "    @tf.Module.with_name_scope\n",
    "    def __call__(self, inp, targ):\n",
    "        encoder_emb_output, encoder_pad_mask = self.embedding_encoder(inp, masking=True)\n",
    "        encoder_output = self.encoder(encoder_emb_output, mask=encoder_pad_mask)\n",
    "        \n",
    "        decoder_emb_output, decoder_pad_mask = self.embedding_decoder(targ, masking=True)\n",
    "        decoder_output, att_weights = self.decoder(decoder_emb_output, encoder_output, mask=decoder_pad_mask)\n",
    "\n",
    "        return self.dense(decoder_output), att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca3107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, num_heads, ffn_dim, embed_dim, vocab_size, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33649e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer(tf.Variable([tf.ones(200)], dtype=tf.int32), tf.Variable([tf.ones(200)], dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187f421",
   "metadata": {},
   "source": [
    "Open and read the dataset file. In my case, there was a simple text splitted by [answ] token, to separate the sentences of questions from the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ca1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"datasets/dataset.txt\",\"r\").read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf1b5e0",
   "metadata": {},
   "source": [
    "Create question-answer pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68917da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "for line in data:\n",
    "    try:\n",
    "        qe, ans = line.split(\"[answ]\")\n",
    "        text_pairs.append((qe, ans))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0310323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3092e",
   "metadata": {},
   "source": [
    "Open a BPE trained model, a model vocabulary created from a text dataset opened a couple of cells ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_model = open(\"bpe_model.model\", \"rb\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853aa94",
   "metadata": {},
   "source": [
    "Create tensorflow-text tokenizers from BPE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a03e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_nonpack = text.SentencepieceTokenizer(bpe_model, out_type=\"int32\", add_bos=False, add_eos=False)\n",
    "tokenizer_pack = text.SentencepieceTokenizer(bpe_model, out_type=\"int32\", add_bos=True, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643508e",
   "metadata": {},
   "source": [
    "Split text pairs into two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_texts = [pair[0] for pair in text_pairs]\n",
    "ans_texts = [pair[1] for pair in text_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd8d49",
   "metadata": {},
   "source": [
    "Dataset tokenization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a379fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x, pack=False):\n",
    "    if not pack:\n",
    "        outputs = tokenizer_nonpack.tokenize(x)\n",
    "        outputs = text.pad_model_inputs(outputs, maxlen, pad_value=0)\n",
    "    else:\n",
    "        outputs = tokenizer_pack.tokenize(x)\n",
    "        outputs = text.pad_model_inputs(outputs, maxlen+1, pad_value=0)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ds(x, y):\n",
    "    q = process(x)[0]\n",
    "    a = process(y, pack=True)[0]\n",
    "    return ({\"encoder_inputs\": q, \"decoder_inputs\": a[:, :-1],}, a[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a610bcc6",
   "metadata": {},
   "source": [
    "Create dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((list(q_texts), list(ans_texts))).shuffle(2048).batch(BATCH_SIZE).map(format_ds, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a1523",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac56394",
   "metadata": {},
   "source": [
    "Learning rate decay function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lr_decay(tf.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, embed_dim, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        a1 = tf.math.rsqrt(step)\n",
    "        a2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(a1,a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ef565",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = lr_decay(embed_dim, 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366704de",
   "metadata": {},
   "source": [
    "Perplexity metric class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7829dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity(tf.metrics.Metric):\n",
    "    def __init__(self, mask_token_id=0):\n",
    "        super().__init__(dtype=tf.float32)\n",
    "        \n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.crossentropy = tf.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')\n",
    "        self.aggregate_crossentropy = tf.Variable(tf.zeros(shape=()))\n",
    "        self.samples_number = tf.Variable(tf.zeros(shape=()))\n",
    "        \n",
    "    def result(self):\n",
    "        if self.samples_number == 0:\n",
    "            return 0.0\n",
    "        perplexity_score = tf.exp(self.aggregate_crossentropy / self.samples_number)\n",
    "        return perplexity_score\n",
    "    \n",
    "    def update_state(self, true, pred):\n",
    "        batch_size = tf.cast(tf.shape(true)[0], tf.float32)\n",
    "        \n",
    "        mask = tf.cast(tf.math.logical_not(tf.equal(true, self.mask_token_id)),tf.float32)\n",
    "\n",
    "        crossentropy_value = tf.cast(self.crossentropy(true, pred, sample_weight=mask),tf.float32) \n",
    "        crossentropy_value = crossentropy_value / tf.reduce_sum(mask)\n",
    "        self.aggregate_crossentropy.assign_add(batch_size * crossentropy_value)\n",
    "        self.samples_number.assign_add(batch_size)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.aggregate_crossentropy.assign(0.0)\n",
    "        self.samples_number.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ea3fb",
   "metadata": {},
   "source": [
    "Setup losses, metrics, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad315462",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = tf.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "train_loss = tf.metrics.Mean()\n",
    "train_ppl = tf.metrics.Mean()\n",
    "ppl_obj = Perplexity()\n",
    "optimizer = tf.optimizers.Adam(epsilon=1e-9, learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0290b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_obj(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def ppl_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    ppl = ppl_obj(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, ppl.dtype)\n",
    "    ppl *= mask\n",
    "    \n",
    "    return tf.reduce_sum(ppl)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(shape=(None, maxlen), dtype=tf.int32), \n",
    "                              tf.TensorSpec(shape=(None, maxlen), dtype=tf.int32),\n",
    "                              tf.TensorSpec(shape=(None, maxlen), dtype=tf.int32)])\n",
    "def train_step(inp, targ, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, _ = transformer.__call__(inp, targ)\n",
    "        loss_val = loss_function(y, logits)\n",
    "        \n",
    "    grads = tape.gradient(loss_val, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, transformer.trainable_variables))\n",
    "            \n",
    "    train_loss(loss_val)\n",
    "    train_ppl(ppl_function(y, logits))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf510d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary = len(train_ds)\n",
    "for e in range(epochs):\n",
    "    train_loss.reset_state()\n",
    "    train_ppl.reset_state()\n",
    "    iterator = iter(train_ds)\n",
    "    step = 0\n",
    "    for i in range(len(train_ds)):\n",
    "        x, y = iterator.get_next()\n",
    "        y = tf.cast(y, tf.int32)\n",
    "        train_step(x['encoder_inputs'], x['decoder_inputs'],y)\n",
    "        step = step + 1\n",
    "\n",
    "        print(\"Epoch \"+str(e) + \"/\" + str(epochs) +\" Batch: \"+str(step) + \"/\" + str(summary) + \" loss: \"+ str(train_loss.result().numpy()) + \" perplexity: \"+str(train_ppl.result().numpy()), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29961dba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(transformer, \"transformer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
