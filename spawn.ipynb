{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport tensorflow_text as text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_layers = 4\nnum_heads = 8\nffn_dim = 2048\nembed_dim = 512\nvocab_size = 31000\nmaxlen = 200\ndataset_file = \"datasets/dataset.txt\"\nbpe_model = \"bpe_model.model\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simple linear layer:","metadata":{}},{"cell_type":"code","source":"class Linear(tf.Module):\n    def __init__(self, input_dim, output_dim, name=\"Linear\"):\n        super().__init__(name=name)\n        self.initializer = tf.initializers.GlorotUniform()\n        self.w = tf.Variable(self.initializer(shape=[input_dim, output_dim]), name=name + \"_w\")\n        self.b = tf.Variable(tf.zeros([output_dim]), name=name+\"_b\")\n        \n    @tf.function\n    @tf.Module.with_name_scope\n    def __call__(self, x):\n        return tf.matmul(x, self.w) + self.b","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gaussan Error Linear Unit activation for FFN.\nSame as tf.keras.activations.Gelu(x, approximate=True):","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef gelu_new(x):\n    return 0.5*x*(1+tf.math.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.math.pow(x, 3))))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create triangular mask for decoder layer, to make sure its attention uses only previous tokens:","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef CasualMask(size): \n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) \n    return mask","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"FeedForwardNetwork. Make transformer block attention outputs fit to the input of the next transformer block:","metadata":{}},{"cell_type":"code","source":"class FFN(tf.Module):\n    def __init__(self, embed_dim, ffn_dim, name=\"FeedForward\"):\n        super().__init__(name=name)\n        self.dense0 = Linear(embed_dim, ffn_dim, name=name)\n        self.dense1 = Linear(ffn_dim, embed_dim, name=name)\n        \n    @tf.function\n    @tf.Module.with_name_scope\n    def __call__(self, x):\n        out1 = self.dense0(x)\n        out2 = gelu_new(out1)\n        return self.dense1(out2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate attention outputs and weights. The mask is multiplied by -1e9 before softmax to assign zero weights for useless tokens that are marked with 1. Setting weights after softmax breaks the probability distribution.","metadata":{}},{"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask=None):\n    qk = tf.matmul(q, k, transpose_b=True)\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    attention_logits = qk / tf.math.sqrt(dk)\n    \n    if mask is not None:\n        attention_logits += (mask * -1e9)\n        \n    attention_weights = tf.nn.softmax(attention_logits, axis=-1)\n    output = tf.matmul(attention_weights, v)\n    return output, attention_weights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Multi Head Attention layer. This layer splits the Q, K, V into separate heads. Each value has a dimension divided by the number of heads after splitting.","metadata":{}},{"cell_type":"code","source":"class MHA(tf.Module):\n    def __init__(self, num_heads, key_dim, name=\"MHA\"):\n        super().__init__(name=name)\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n        self.depth = key_dim // num_heads\n        \n        self.wq = Linear(key_dim, key_dim, name=\"MHA_query\")\n        self.wk = Linear(key_dim, key_dim, name=\"MHA_key\")\n        self.wv = Linear(key_dim, key_dim, name=\"MHA_value\")\n        \n        self.dense = Linear(key_dim, key_dim, name=\"MHA_dense\")\n        \n    @tf.function\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, - 1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0,2,1,3])\n    \n    @tf.function\n    @tf.Module.with_name_scope\n    def __call__(self, q, k, v, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n        \n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n        \n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        \n        scaled_attention = tf.transpose(scaled_attention, perm=[0,2,1,3])\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.key_dim))\n        \n        output = self.dense(concat_attention)\n        \n        return output, attention_weights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Token and postitional embedding. If it is needed, this layer also returns the padding mask used in the scaled dot product attention function.","metadata":{}},{"cell_type":"code","source":"def get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n    return pos * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Embedding(tf.Module):\n    def __init__(self, vocab_size, embed_dim, maxlen, name=\"Embeddings\"):\n        super().__init__(name=name)\n        self.embed_dim = embed_dim\n        self.initializer = tf.initializers.GlorotUniform()\n        \n        self.w0 = tf.Variable(self.initializer([vocab_size, embed_dim]),name=\"token_embedding\")\n        self.pos_encoding = positional_encoding(maxlen,self.embed_dim)\n        \n    @tf.function\n    @tf.Module.with_name_scope\n    def __call__(self, x, masking=False):\n        x = tf.cast(x, tf.int32)\n        seq_len = tf.shape(x)[-1]\n        \n        if isinstance(x, tf.sparse.SparseTensor):\n            sparse_inputs_expanded = tf.sparse.expand_dims(x, axis=-1)\n            out = tf.nn.safe_embedding_lookup_sparse(embedding_weights=self.w0,sparse_ids=sparse_inputs_expanded, default_id=0)\n            \n        else:\n            out = tf.nn.embedding_lookup(self.w0, x)\n\n        out *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n        out += self.pos_encoding[:, :seq_len, :]\n        \n        if masking:\n            #Create mask to prevent encoder paying attention to PAD tokens\n            mask = tf.cast(tf.math.equal(x, 0)[:, tf.newaxis, tf.newaxis, :], tf.float32)\n            return out, mask\n        \n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Layer normalization. Normalize to mean = 0, std = 1.","metadata":{}},{"cell_type":"code","source":"class LayerNorm(tf.Module):\n    def __init__(self, shape, epsilon=1e-5, axis=-1, name=\"LayerNorm\"):\n        super().__init__(name=name)\n        self.gamma = None\n        self.beta = None\n        self.epsilon = epsilon\n        self.axis = axis\n        self.gamma = tf.Variable(tf.ones(shape), name=\"layernorm_gamma\")\n        self.beta = tf.Variable(tf.zeros(shape), name=\"layernorm_beta\")\n    \n    @tf.function\n    @tf.Module.with_name_scope\n    def __call__(self, x):\n        u = tf.reduce_mean(x, axis=self.axis, keepdims=True)\n        s = tf.reduce_mean(tf.square(x-u), axis=self.axis, keepdims=True)\n        x = (x - u) * tf.math.rsqrt(s + self.epsilon)\n        return x * self.gamma + self.beta","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create encoder block:","metadata":{}},{"cell_type":"code","source":"class Encoder(tf.Module):\n    def __init__(self, num_heads, embed_dim, ffn_dim, name=\"encoder\"):\n        super().__init__(name=name)\n        self.att = MHA(num_heads, embed_dim)\n        self.ffn = FFN(embed_dim, ffn_dim)\n        self.norm1 = LayerNorm(embed_dim)\n        self.norm2 = LayerNorm(embed_dim)\n        \n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.ffn_dim = ffn_dim\n        \n    @tf.function\n    @tf.Module.with_name_scope\n    def __call__(self, x, mask=None):\n        att_output, att_weights = self.att(x,x,x, mask)\n        att_output = tf.nn.dropout(att_output, rate=0.1)\n        norm_output = self.norm1(x + att_output)\n        ffn_out = self.ffn(norm_output)\n        ffn_out = tf.nn.dropout(ffn_out, rate=0.1)\n        \n        return self.norm2(norm_output + ffn_out)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create decoder block:","metadata":{}},{"cell_type":"code","source":"class Decoder(tf.Module):\n    def __init__(self, num_heads, embed_dim, ffn_dim, name=\"decoder\"):\n        super().__init__(name=name)\n        self.att1 = MHA(num_heads, embed_dim)\n        self.att2 = MHA(num_heads, embed_dim)\n        self.ffn = FFN(embed_dim, ffn_dim)\n        \n        self.layernorm1 = LayerNorm(embed_dim)\n        self.layernorm2 = LayerNorm(embed_dim)\n        self.layernorm3 = LayerNorm(embed_dim)\n        \n    @tf.function\n    @tf.Module.with_name_scope\n    def __call__(self, x, encoder_seq, mask=None):\n        seq_len = tf.shape(x)[1]\n        \n        casual_mask = CasualMask(seq_len)\n        \n        if mask is not None:\n            mask = tf.maximum(tf.cast(mask, tf.float32), casual_mask)\n            \n        att1_output, _ = self.att1(x, x, x, casual_mask)\n        att1_dropout = tf.nn.dropout(att1_output, rate=0.1)\n        out1 = self.layernorm1(x+att1_dropout)\n        \n        att2_output, _ = self.att2(out1, encoder_seq, encoder_seq, mask)\n        att2_dropout = tf.nn.dropout(att2_output, rate=0.1)\n        out2 = self.layernorm2(out1 + att2_dropout)\n        \n        ffn_output = self.ffn(out2)\n        ffn_dropout = tf.nn.dropout(ffn_output, rate=0.1)\n        return self.layernorm3 (out2+ffn_dropout)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These layers makes it possible to use multiple encoder and decoder layers by iterating over the specified number of layers.","metadata":{}},{"cell_type":"code","source":"class Encoder_layer(tf.Module):\n    def __init__(self, num_layers, num_heads, embed_dim, ffn_dim, name=\"encoder_layer\"):\n        super().__init__(name=name)\n        self.num_layers = num_layers\n        self.enc_layers = [Encoder(num_heads, embed_dim, ffn_dim, name=\"encoder\"+str(_)) #Every block needs a name\n                    for _ in range(num_layers)]                                          #Or it will not be recognized\n                                                                                         #After loading model from \n                                                                                         #tf.saved_model\n    @tf.function\n    @tf.Module.with_name_scope\n    def __call__(self, x, mask=None):\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, mask)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder_layer(tf.Module):\n    def __init__(self, num_layers, num_heads, embed_dim, ffn_dim, name=\"decoder_layer\"):\n        super().__init__(name=name)\n        self.num_layers = num_layers\n        self.dec_layers = [Decoder(num_heads, embed_dim, ffn_dim, name=\"decoder\"+str(_))\n                          for _ in range(num_layers)]\n        \n    @tf.function\n    @tf.Module.with_name_scope\n    def __call__(self, x, encoder_seq, mask=None):\n        for i in range(self.num_layers):\n            x = self.dec_layers[i](x, encoder_seq, mask)\n            \n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define hyperparameters for building the model","metadata":{}},{"cell_type":"markdown","source":"Build the transformer model:","metadata":{}},{"cell_type":"code","source":"class Transformer(tf.Module):\n    def __init__(self, num_layers, num_heads, ffn_dim, embed_dim, vocab_size, maxlen, name=\"transformer\"):\n        super().__init__(name=name)\n        \n        self.maxlen = maxlen\n        self.vocab_size = vocab_size\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.ffn_dim = ffn_dim\n        self.num_layers = num_layers\n        \n        self.dense = Linear(embed_dim, vocab_size, name=\"model_output\")\n        \n        self.embedding_encoder = Embedding(vocab_size, embed_dim, maxlen)\n        self.embedding_decoder = Embedding(vocab_size, embed_dim, maxlen)\n        \n        self.encoder = Encoder_layer(num_layers, num_heads, embed_dim, ffn_dim)\n        self.decoder = Decoder_layer(num_layers, num_heads, embed_dim, ffn_dim)\n    \n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, maxlen], dtype=tf.int32), tf.TensorSpec(shape=[None,maxlen], dtype=tf.int32)])\n    @tf.Module.with_name_scope\n    def __call__(self, inp, targ):\n        encoder_emb_output, encoder_pad_mask = self.embedding_encoder(inp, masking=True)\n        encoder_output = self.encoder(encoder_emb_output, mask=encoder_pad_mask)\n        \n        decoder_emb_output, decoder_pad_mask = self.embedding_decoder(targ, masking=True)\n        decoder_output = self.decoder(decoder_emb_output, encoder_output, mask=decoder_pad_mask)\n\n        return self.dense(decoder_output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = Transformer(num_layers, num_heads, ffn_dim, embed_dim, vocab_size, maxlen)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer(tf.Variable([tf.ones(maxlen)], dtype=tf.int32), tf.Variable([tf.ones(maxlen)], dtype=tf.int32))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Open and read the dataset file. In my case, there was a simple text splitted by [answ] token, to separate the sentences of questions from the answers.","metadata":{}},{"cell_type":"code","source":"data = open(dataset_file,\"r\").read().split(\"\\n\")[:-1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create question-answer pairs","metadata":{}},{"cell_type":"code","source":"text_pairs = []\nfor line in data:\n    try:\n        qe, ans = line.split(\"[answ]\")\n        text_pairs.append((qe, ans))\n    except:\n        pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandom.shuffle(text_pairs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Open a BPE trained model, a model vocabulary created from a text dataset opened a couple of cells ago","metadata":{}},{"cell_type":"code","source":"bpe_model = open(bpe_model, \"rb\").read()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create tensorflow-text tokenizers from BPE model","metadata":{}},{"cell_type":"code","source":"tokenizer_nonpack = text.SentencepieceTokenizer(bpe_model, out_type=\"int32\", add_bos=False, add_eos=False)\ntokenizer_pack = text.SentencepieceTokenizer(bpe_model, out_type=\"int32\", add_bos=True, add_eos=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 25","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split text pairs into two lists","metadata":{}},{"cell_type":"code","source":"q_texts = [pair[0] for pair in text_pairs]\nans_texts = [pair[1] for pair in text_pairs]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset tokenization function:","metadata":{}},{"cell_type":"code","source":"def process(x, pack=False):\n    if not pack:\n        outputs = tokenizer_nonpack.tokenize(x)\n        outputs = text.pad_model_inputs(outputs, maxlen, pad_value=0)\n    else:\n        outputs = tokenizer_pack.tokenize(x)\n        outputs = text.pad_model_inputs(outputs, maxlen+1, pad_value=0)\n    return outputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_ds(x, y):\n    q = process(x)[0]\n    a = process(y, pack=True)[0]\n    return ({\"encoder_inputs\": q, \"decoder_inputs\": a[:, :-1],}, a[:, 1:])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create dataset:","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((list(q_texts), list(ans_texts))).shuffle(2048).batch(BATCH_SIZE).map(format_ds, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Learning rate decay function:","metadata":{}},{"cell_type":"code","source":"class lr_decay(tf.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, embed_dim, warmup_steps):\n        super().__init__()\n        self.warmup_steps = warmup_steps\n        self.embed_dim = tf.cast(embed_dim, tf.float32)\n\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)\n        a1 = tf.math.rsqrt(step)\n        a2 = step * (self.warmup_steps ** -1.5)\n        \n        return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(a1,a2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_schedule = lr_decay(embed_dim, 4000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perplexity metric class:","metadata":{}},{"cell_type":"code","source":"class Perplexity(tf.metrics.Metric):\n    def __init__(self, mask_token_id=0):\n        super().__init__(dtype=tf.float32)\n        \n        self.mask_token_id = mask_token_id\n        self.crossentropy = tf.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')\n        self.aggregate_crossentropy = tf.Variable(tf.zeros(shape=()))\n        self.samples_number = tf.Variable(tf.zeros(shape=()))\n        \n    def result(self):\n        if self.samples_number == 0:\n            return 0.0\n        perplexity_score = tf.exp(self.aggregate_crossentropy / self.samples_number)\n        return perplexity_score\n    \n    def update_state(self, true, pred):\n        batch_size = tf.cast(tf.shape(true)[0], tf.float32)\n        \n        mask = tf.cast(tf.math.logical_not(tf.equal(true, self.mask_token_id)),tf.float32)\n\n        crossentropy_value = tf.cast(self.crossentropy(true, pred, sample_weight=mask),tf.float32) \n        crossentropy_value = crossentropy_value / tf.reduce_sum(mask)\n        self.aggregate_crossentropy.assign_add(batch_size * crossentropy_value)\n        self.samples_number.assign_add(batch_size)\n        \n    def reset_state(self):\n        self.aggregate_crossentropy.assign(0.0)\n        self.samples_number.assign(0.0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setup losses, metrics, and optimizer","metadata":{}},{"cell_type":"code","source":"loss_obj = tf.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ntrain_loss = tf.metrics.Mean()\ntrain_ppl = tf.metrics.Mean()\nppl_obj = Perplexity()\noptimizer = tf.optimizers.Adam(epsilon=1e-9, learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_obj(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n\n\ndef ppl_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real,0))\n    ppl = ppl_obj(real, pred)\n    \n    mask = tf.cast(mask, ppl.dtype)\n    ppl *= mask\n    \n    return tf.reduce_sum(ppl)/tf.reduce_sum(mask)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function(input_signature=[tf.TensorSpec(shape=(None, maxlen), dtype=tf.int32), \n                              tf.TensorSpec(shape=(None, maxlen), dtype=tf.int32),\n                              tf.TensorSpec(shape=(None, maxlen), dtype=tf.int32)])\ndef train_step(inp, targ, y):\n    with tf.GradientTape() as tape:\n        logits = transformer.__call__(inp, targ)\n        loss_val = loss_function(y, logits)\n        \n    grads = tape.gradient(loss_val, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(grads, transformer.trainable_variables))\n            \n    train_loss(loss_val)\n    train_ppl(ppl_function(y, logits))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary = len(train_ds)\nfor e in range(epochs):\n    train_loss.reset_state()\n    train_ppl.reset_state()\n    iterator = iter(train_ds)\n    step = 0\n    for i in range(len(train_ds)):\n        x, y = iterator.get_next()\n        y = tf.cast(y, tf.int32)\n        train_step(x['encoder_inputs'], x['decoder_inputs'],y)\n        step = step + 1\n\n        print(\"Epoch \"+str(e) + \"/\" + str(epochs) +\" Batch: \"+str(step) + \"/\" + str(summary) + \" loss: \"+ str(train_loss.result().numpy()) + \" perplexity: \"+str(train_ppl.result().numpy()), end=\"\\r\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.saved_model.save(transformer, \"transformer\", signatures={'call':transformer.__call__})","metadata":{},"execution_count":null,"outputs":[]}]}